# ğŸ™ï¸ Non-Verbal Audio Gestures Recognition

Sistema de reconhecimento de gestos nÃ£o-verbais por Ã¡udio usando Deep Learning. O projeto identifica sons como assobios e estalos de dedos em tempo real atravÃ©s de uma interface grÃ¡fica intuitiva.

## ğŸš€ CaracterÃ­sticas

- **ClassificaÃ§Ã£o de Ãudio em Tempo Real**: Reconhece gestos nÃ£o-verbais atravÃ©s do microfone
- **Interface GrÃ¡fica**: Push-to-talk simples e responsiva usando Tkinter
- **CNN Personalizada**: Modelo de Convolutional Neural Network treinado em mel-spectrogramas
- **Data Augmentation**: Time-shifting, pitch-shifting e adiÃ§Ã£o de ruÃ­do para melhorar generalizaÃ§Ã£o
- **Arquitetura Modular**: CÃ³digo organizado e reutilizÃ¡vel

## ğŸ“‹ PrÃ©-requisitos

- Python 3.8+
- Miniconda/Anaconda (opcional, mas recomendado)
- Microfone funcional
- PortAudio (para sounddevice)

### InstalaÃ§Ã£o do PortAudio (Linux)

```bash
# Ubuntu/Debian
sudo apt-get install portaudio19-dev
```

## ğŸ”§ InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**
```bash
git clone https://github.com/VictorAGomes/nonverbal-audio-gestures.git
cd nonverbal-audio-gestures
```

2. **Crie e ative o ambiente virtual**
```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

3. **Instale as dependÃªncias**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

## ğŸ“‚ Estrutura do Projeto

```
nonverbal-audio-gestures/
â”œâ”€â”€ config.py                 # ConfiguraÃ§Ãµes globais centralizadas
â”œâ”€â”€ model.py                  # Arquitetura CNN (NonVerbalCNN)
â”œâ”€â”€ spectrogram_utils.py      # ConversÃ£o de espectrograma para imagem
â”œâ”€â”€ train.py                  # Script de treinamento do modelo
â”œâ”€â”€ gui_app.py                # Interface grÃ¡fica para inferÃªncia
â”œâ”€â”€ main.py                   # Script de teste de mel-spectrograma
â”œâ”€â”€ augmentation.py           # FunÃ§Ãµes de data augmentation
â”œâ”€â”€ requirements.txt          # DependÃªncias do projeto
â”œâ”€â”€ best_nonverbal_model.pth  # Modelo treinado (apÃ³s treinamento)
â””â”€â”€ data/                     # Dados de treinamento
    â”œâ”€â”€ assobio/              # Ãudios de assobio
    â”œâ”€â”€ dedo/                 # Ãudios de estalo de dedos
    â””â”€â”€ palma/                # Ãudios de palmas (opcional)
```

## ğŸ¯ Como Usar

### 1. Preparar os Dados

Organize seus arquivos de Ã¡udio na estrutura:

```
data/
â”œâ”€â”€ assobio/
â”‚   â”œâ”€â”€ audio1.wav
â”‚   â”œâ”€â”€ audio2.opus
â”‚   â””â”€â”€ ...
â”œâ”€â”€ dedo/
â”‚   â”œâ”€â”€ audio1.wav
â”‚   â”œâ”€â”€ audio2.mp3
â”‚   â””â”€â”€ ...
â””â”€â”€ palma/  # Opcional
    â””â”€â”€ ...
```

Formatos suportados: `.wav`, `.mp3`, `.opus`, `.flac`, `.m4a`, `.ogg`

### 2. Treinar o Modelo

```bash
python train.py
```

O script irÃ¡:
- Carregar os dados da pasta `data/`
- Aplicar data augmentation
- Treinar a CNN por 50 Ã©pocas
- Salvar o melhor modelo como `best_nonverbal_model.pth`
- Gerar matriz de confusÃ£o (`confusion_matrix.png`)

### 3. Executar a Interface GrÃ¡fica

```bash
python gui_app.py
```

**Como usar a interface:**
1. Pressione e **segure** o botÃ£o ğŸ™ï¸
2. FaÃ§a o som (assobio ou estalo de dedos)
3. **Solte** o botÃ£o
4. Aguarde o resultado aparecer na tela

## ğŸ§  Arquitetura do Modelo

### NonVerbalCNN
```
Conv2D (3â†’32) â†’ ReLU â†’ MaxPool
Conv2D (32â†’64) â†’ ReLU â†’ MaxPool
Conv2D (64â†’128) â†’ ReLU â†’ MaxPool
Conv2D (128â†’256) â†’ ReLU â†’ MaxPool
Flatten
Dropout(0.3) â†’ FC(16384â†’512) â†’ ReLU
Dropout(0.5) â†’ FC(512â†’num_classes)
```

### Pipeline de Processamento
1. **Ãudio** (16kHz, mono, 1 segundo)
2. **Mel-Spectrograma** (128 bandas mel)
3. **ConversÃ£o para Imagem RGB** (128Ã—128)
4. **NormalizaÃ§Ã£o** [0, 1]
5. **InferÃªncia CNN**

## âš™ï¸ ConfiguraÃ§Ãµes (config.py)

```python
SR = 16000              # Sample rate
DURATION = 1.0          # DuraÃ§Ã£o em segundos
N_FFT = 1024            # Tamanho FFT
HOP_LENGTH = 256        # Passo entre janelas
N_MELS = 128            # Bandas Mel
IMG_SIZE = (128, 128)   # Tamanho da imagem
CLASSES = ['assobio', 'dedo']  # Classes a serem reconhecidas
```

## ğŸ“Š Data Augmentation

O treinamento aplica as seguintes tÃ©cnicas:
- **Time Shifting** (70% chance): Desloca o sinal no tempo
- **Pitch Shifting** (50% chance): Altera a tonalidade
- **Background Noise** (80% chance): Adiciona ruÃ­do gaussiano

## ğŸ” Testando Mel-Spectrograma

Para visualizar como o Ã¡udio Ã© convertido:

```bash
python main.py
```

## ğŸ“ˆ MÃ©tricas de AvaliaÃ§Ã£o

ApÃ³s o treinamento, sÃ£o gerados:
- **AcurÃ¡cia de Teste**: Porcentagem de acertos no conjunto de teste
- **Matriz de ConfusÃ£o**: VisualizaÃ§Ã£o de erros de classificaÃ§Ã£o
- **Classification Report**: Precision, Recall e F1-Score por classe

## ğŸ› ï¸ Desenvolvimento

### Adicionar Nova Classe

1. Crie uma pasta em `data/` com o nome da classe
2. Adicione arquivos de Ã¡udio nessa pasta
3. Atualize `config.py`:
```python
CLASSES = ['assobio', 'dedo', 'nova_classe']
```
4. Retreine o modelo: `python train.py`

### Estrutura Modular

- **`config.py`**: Centralize todas as constantes aqui
- **`model.py`**: Defina novas arquiteturas aqui
- **`spectrogram_utils.py`**: FunÃ§Ãµes de processamento de Ã¡udio
- **`train.py`**: Pipeline de treinamento


## ğŸ‘¥ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para:
1. Fazer fork do projeto
2. Criar uma branch para sua feature (`git checkout -b feature/NovaFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona NovaFeature'`)
4. Push para a branch (`git push origin feature/NovaFeature`)
5. Abrir um Pull Request

## ğŸ“§ Contato

DÃºvidas ou sugestÃµes? Entre em contato atravÃ©s do GitHub.

---

